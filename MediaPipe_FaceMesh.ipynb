{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a930d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd77860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACE_CONNECTIONS is renamed to FACEMESH_CONTOURS\n",
    "lipsUpperOuter: [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "lipsLowerOuter: [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "lipsUpperInner: [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
    "lipsLowerInner: [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
    "rightEyeUpper0: [246, 161, 160, 159, 158, 157, 173]\n",
    "rightEyeLower0: [33, 7, 163, 144, 145, 153, 154, 155, 133]\n",
    "rightEyeUpper1: [247, 30, 29, 27, 28, 56, 190]\n",
    "rightEyeLower1: [130, 25, 110, 24, 23, 22, 26, 112, 243]\n",
    "rightEyeUpper2: [113, 225, 224, 223, 222, 221, 189]\n",
    "rightEyeLower2: [226, 31, 228, 229, 230, 231, 232, 233, 244]\n",
    "rightEyeLower3: [143, 111, 117, 118, 119, 120, 121, 128, 245]\n",
    "rightEyebrowUpper: [156, 70, 63, 105, 66, 107, 55, 193]\n",
    "rightEyebrowLower: [35, 124, 46, 53, 52, 65]\n",
    "rightEyeIris: [473, 474, 475, 476, 477]\n",
    "leftEyeUpper0: [466, 388, 387, 386, 385, 384, 398]\n",
    "leftEyeLower0: [263, 249, 390, 373, 374, 380, 381, 382, 362]\n",
    "leftEyeUpper1: [467, 260, 259, 257, 258, 286, 414]\n",
    "leftEyeLower1: [359, 255, 339, 254, 253, 252, 256, 341, 463]\n",
    "leftEyeUpper2: [342, 445, 444, 443, 442, 441, 413]\n",
    "leftEyeLower2: [446, 261, 448, 449, 450, 451, 452, 453, 464]\n",
    "leftEyeLower3: [372, 340, 346, 347, 348, 349, 350, 357, 465]\n",
    "leftEyebrowUpper: [383, 300, 293, 334, 296, 336, 285, 417]\n",
    "leftEyebrowLower: [265, 353, 276, 283, 282, 295]\n",
    "leftEyeIris: [468, 469, 470, 471, 472]\n",
    "\n",
    "eye_list = [33, 246, 7, 161, 163, 160, 144, 159, 145, 158, 153, 157, 154, 173, 155, 133]\n",
    "\n",
    "#connections=mp_face_mesh.FACEMESH_TESSELATION\n",
    "#mp_face_mesh.FACEMESH_FACE_OVAL\n",
    "#mp_face_mesh.FACEMESH_LEFT_EYE\n",
    "#mp_face_mesh.FACEMESH_LEFT_EYEBROW\n",
    "#mp_face_mesh.FACEMESH_LIPS\n",
    "#mp_face_mesh.FACEMESH_RIGHT_EYE\n",
    "#mp_face_mesh.FACEMESH_RIGHT_EYEBROW\n",
    "#mp_face_mesh.FACEMESH_TESSELATION\n",
    "#mp_face_mesh.FACEMESH_CONTOURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb734483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_image_alpha(img, img_overlay, x, y, alpha_mask):\n",
    "    # Image ranges\n",
    "    y1, y2 = max(0, y), min(img.shape[0], y + img_overlay.shape[0])\n",
    "    x1, x2 = max(0, x), min(img.shape[1], x + img_overlay.shape[1])\n",
    "\n",
    "    # Overlay ranges\n",
    "    y1o, y2o = max(0, -y), min(img_overlay.shape[0], img.shape[0] - y)\n",
    "    x1o, x2o = max(0, -x), min(img_overlay.shape[1], img.shape[1] - x)\n",
    "\n",
    "    # Exit if nothing to do\n",
    "    if y1 >= y2 or x1 >= x2 or y1o >= y2o or x1o >= x2o:\n",
    "        return\n",
    "\n",
    "    # Blend overlay within the determined ranges\n",
    "    img_crop = img[y1:y2, x1:x2]\n",
    "    img_overlay_crop = img_overlay[y1o:y2o, x1o:x2o]\n",
    "    alpha = alpha_mask[y1o:y2o, x1o:x2o, np.newaxis]\n",
    "    alpha_inv = 1.0 - alpha\n",
    "\n",
    "    img_crop[:] = alpha * img_overlay_crop + alpha_inv * img_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54434dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawingModule = mediapipe.solutions.drawing_utils\n",
    "faceModule = mediapipe.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0296ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "circleDrawingSpec = drawingModule.DrawingSpec(thickness=1, circle_radius=1, color=(100,75,75))\n",
    "lineDrawingSpec = drawingModule.DrawingSpec(thickness=1, color=(0,255,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14591b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Image\n",
    "#with faceModule.FaceMesh(static_image_mode=True) as face:\n",
    "#    # Face landmarks estimation\n",
    "#    # From image\n",
    "#    image = cv2.imread(\"D:/Research/opencv/image/MediaPipe/sung.jpg\")\n",
    "    \n",
    "#    results = face.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "#    if results.multi_face_landmarks != None:\n",
    "#        for faceLandmarks in results.multi_face_landmarks:\n",
    "#            drawingModule.draw_landmarks(image, faceLandmarks, faceModule.FACEMESH_CONTOURS, circleDrawingSpec, lineDrawingSpec)\n",
    "#    cv2.imshow('Test image', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d00d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_img = cv2.imread(\"D:/Research/opencv/image/MediaPipe/Sharingan_triple.svg.png\")\n",
    "#alpha_mask = eye_img[:, :, 2] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85451258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "with faceModule.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        h, w, d = image.shape\n",
    "        results = face.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        #if results.multi_face_landmarks != None:\n",
    "        #    for faceLandmarks in results.multi_face_landmarks:\n",
    "        #        drawingModule.draw_landmarks(image, faceLandmarks, faceModule.FACEMESH_CONTOURS, circleDrawingSpec, lineDrawingSpec)\n",
    "        #        #drawingModule.draw_landmarks(image, faceLandmarks, faceModule.FACEMESH_LEFT_EYE | faceModule.FACEMESH_RIGHT_EYE, None, lineDrawingSpec)                \n",
    "        #        #overlay_image_alpha(image, eye_img, 0, 0, alpha_mask)\n",
    "        \n",
    "        if results.multi_face_landmarks != None:\n",
    "            # 歷遍左眼各點取得目前座標與眼睛大小\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                #drawingModule.draw_landmarks(image, face_landmarks, faceModule.FACEMESH_LEFT_EYE | faceModule.FACEMESH_RIGHT_EYE, None, lineDrawingSpec)\n",
    "                #break\n",
    "                eye_point = []\n",
    "                eye_size = []\n",
    "                for index in eye_list:\n",
    "                    x = int(face_landmarks.landmark[index].x * w)\n",
    "                    y = int(face_landmarks.landmark[index].y * h)\n",
    "                    eye_point.append([x, y])\n",
    "                    if index == 153 or index == 159:\n",
    "                        eye_size.append([x, y])\n",
    "                if len(eye_size) == 2:\n",
    "                    eye_len = int(math.pow(math.pow((eye_size[0][0] - eye_size[1][0]), 2) + math.pow((eye_size[0][1] - eye_size[1][1]), 2), 0.5))\n",
    "\n",
    "            #try:\n",
    "                # 將取得的各點透過statistics計算出眼睛中心座標\n",
    "                points = eye_point\n",
    "                center = [statistics.mean(i) for i in zip(*points)]\n",
    "\n",
    "               # 透過剛剛取得的眼睛大小, 將眼睛圖案的圖片轉換成適合的大小\n",
    "                eye = cv2.resize(eye_img, (eye_len-6, eye_len-6))\n",
    "\n",
    "                # 透過一系列的處理將眼睛圖片貼在左眼上\n",
    "                eye_gray = cv2.cvtColor(eye, cv2.COLOR_BGR2GRAY)\n",
    "                _, eye_mask = cv2.threshold(eye_gray, 25, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "                img_height, img_width, _ = eye.shape\n",
    "                x, y = int(center[0]-img_width/2), int(center[1]-img_height/2)\n",
    "                eye_area = image[y: y+img_height, x: x+img_width]\n",
    "                eye_area_no_eye = cv2.bitwise_and(eye_area, eye_area, mask=eye_mask)\n",
    "                final_eye = cv2.add(eye_area_no_eye, eye)\n",
    "                image[y: y+img_height, x: x+img_width] = final_eye\n",
    "                \n",
    "                cv2.imshow('MediaPipe Face Mesh', image)\n",
    "            #except:\n",
    "            #    pass\n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "              break     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73e36495",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a585a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
